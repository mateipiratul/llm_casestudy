\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{leighton2003}
\citation{chandra2025}
\citation{cichocka2020}
\citation{guo2024}
\citation{kumar2024}
\citation{lutz2025}
\citation{weissburg2025}
\citation{sun2025}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Bias in Large Language Models}{1}{subsection.2.1}\protected@file@percent }
\citation{oyserman2008}
\citation{hofstede2001}
\citation{tao2024}
\citation{hauser2024}
\citation{bhatia2024}
\citation{zhao2025}
\citation{petrescu}
\citation{coman2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Persona prompting}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Cultural alignment of LLMs}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}{section.3}\protected@file@percent }
\citation{zheng2023}
\citation{deepseekai2025}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}LLM Selection}{3}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces This figure illustrates the timeline of the Large Language Models (LLMs) selected for our study, categorized by their release date.}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:model_timeline}{{1}{3}{This figure illustrates the timeline of the Large Language Models (LLMs) selected for our study, categorized by their release date}{figure.caption.1}{}}
\citation{walker2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Running the queries}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Prompt}{4}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Consistency}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Within the Model}{4}{subsubsection.4.1.1}\protected@file@percent }
\newlabel{fig:model_scale_avg_std}{{2a}{4}{Average standard deviation of Likert-scale responses (1-10) for each model. Lower values indicate higher consistency}{figure.caption.2}{}}
\newlabel{sub@fig:model_scale_avg_std}{{a}{4}{Average standard deviation of Likert-scale responses (1-10) for each model. Lower values indicate higher consistency}{figure.caption.2}{}}
\newlabel{fig:model_judge_avg_std}{{2b}{4}{Average agreement score on scale between the LLM-as-a-judge and the model itself}{figure.caption.2}{}}
\newlabel{sub@fig:model_judge_avg_std}{{b}{4}{Average agreement score on scale between the LLM-as-a-judge and the model itself}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparative consistency metrics for model performance. Figure (a) shows the variability in scaled answers, while Figure (b) shows the judged quality of essay responses.}}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:side_by_side_consistency}{{2}{4}{Comparative consistency metrics for model performance. Figure (a) shows the variability in scaled answers, while Figure (b) shows the judged quality of essay responses}{figure.caption.2}{}}
\citation{gorun2018}
\citation{gulias2016}
\citation{bai2024}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This figure shows the perfect consistency of "Yes" and "No" responses within each model across multiple runs and languages. A perfectly consistent model would always give the same "Yes" or "No" answer for a given question in a given language across all runs.}}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:model_yesno_consistency}{{3}{5}{This figure shows the perfect consistency of "Yes" and "No" responses within each model across multiple runs and languages. A perfectly consistent model would always give the same "Yes" or "No" answer for a given question in a given language across all runs}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Within the Language}{5}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Language agreement with the cross-model consensus for each question. Low scores indicate a strong divergent narrative.}}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:lang_agreement_per_question}{{4}{5}{Language agreement with the cross-model consensus for each question. Low scores indicate a strong divergent narrative}{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Cross-Run Consistency Metrics by Language. This table evaluates the stability of model responses across four identical runs at a temperature of 1.0.}}{5}{table.caption.5}\protected@file@percent }
\newlabel{tab:language_consistency}{{1}{5}{Cross-Run Consistency Metrics by Language. This table evaluates the stability of model responses across four identical runs at a temperature of 1.0}{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Cross-Model \& Cross-Language Analysis}{6}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Detailed matrix of divergence between romanian (RO) and hungarian (HU).}}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:model_scale_consistency_hu}{{5}{6}{Detailed matrix of divergence between romanian (RO) and hungarian (HU)}{figure.caption.6}{}}
\citation{li2025}
\citation{gururangan2022}
\citation{bender2021}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Detailed matrix of response consistency, where each cell visualizes the stability of a specific model's "Yes" answers to a specific question across four runs, further subdivided by the language of the prompt. The color of each quadrant indicates the count of "Yes" responses, providing a granular view of both intra-model consistency and cross-lingual bias.}}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:model_yesno_quadrants}{{6}{7}{Detailed matrix of response consistency, where each cell visualizes the stability of a specific model's "Yes" answers to a specific question across four runs, further subdivided by the language of the prompt. The color of each quadrant indicates the count of "Yes" responses, providing a granular view of both intra-model consistency and cross-lingual bias}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The Meticulous Effect of Temperature}{7}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Effect of Temperature Reduction (1.0 vs. 0.6) on Response Stability by Language. This table quantifies how changing the model's temperature from a creative setting (1.0) to a more deterministic one (0.6) impacts the answers, comparing the stability of the binary choice against the magnitude of the shift in the numeric score.}}{7}{table.caption.8}\protected@file@percent }
\newlabel{tab:temp_effect_lang}{{2}{7}{Effect of Temperature Reduction (1.0 vs. 0.6) on Response Stability by Language. This table quantifies how changing the model's temperature from a creative setting (1.0) to a more deterministic one (0.6) impacts the answers, comparing the stability of the binary choice against the magnitude of the shift in the numeric score}{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Questions with inconsistencies analysis}{7}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{7}{section.5}\protected@file@percent }
\bibstyle{consilr}
\bibdata{llm_casestudy}
\@writefile{toc}{\contentsline {section}{\numberline {6}Future Work}{8}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Appendix}{8}{section.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces List of questions for analysis by ID.}}{8}{table.caption.9}\protected@file@percent }
\gdef \@abspage@last{8}
